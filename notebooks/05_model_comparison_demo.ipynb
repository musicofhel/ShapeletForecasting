{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Development and Comparison Demo\n",
    "\n",
    "This notebook demonstrates the model development pipeline for financial time series prediction using wavelet features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our modules\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.models.sequence_predictor import LSTMModel, GRUModel, TimeSeriesDataset, train_sequence_model\n",
    "from src.models.transformer_predictor import TransformerPredictor\n",
    "from src.models.xgboost_predictor import XGBoostPredictor, XGBoostTimeSeriesCV\n",
    "from src.models.ensemble_model import EnsembleModel\n",
    "from src.models.model_trainer import ModelTrainer, WalkForwardValidator\n",
    "from src.models.model_evaluator import ModelEvaluator, create_evaluation_report\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Synthetic Financial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic financial time series\n",
    "np.random.seed(42)\n",
    "n_samples = 2000\n",
    "n_features = 50  # Simulating wavelet and technical features\n",
    "\n",
    "# Time index\n",
    "dates = pd.date_range(start='2018-01-01', periods=n_samples, freq='D')\n",
    "\n",
    "# Generate base price series with trend, seasonality, and noise\n",
    "trend = np.linspace(100, 150, n_samples)\n",
    "seasonal = 10 * np.sin(2 * np.pi * np.arange(n_samples) / 365.25)\n",
    "noise = 2 * np.random.randn(n_samples)\n",
    "price = trend + seasonal + noise\n",
    "\n",
    "# Add some market regime changes\n",
    "regime_change_points = [500, 1000, 1500]\n",
    "for cp in regime_change_points:\n",
    "    price[cp:] += np.random.randn() * 5\n",
    "\n",
    "# Calculate returns (target variable)\n",
    "returns = np.diff(price) / price[:-1]\n",
    "returns = np.concatenate([[0], returns])  # Pad first return\n",
    "\n",
    "# Generate features (simulating wavelet coefficients and technical indicators)\n",
    "features = np.random.randn(n_samples, n_features)\n",
    "\n",
    "# Add some structure to features\n",
    "# Wavelet-like features at different scales\n",
    "for i in range(5):\n",
    "    scale = 2 ** (i + 1)\n",
    "    features[:, i] = np.convolve(returns, np.ones(scale)/scale, mode='same')\n",
    "\n",
    "# Technical indicator-like features\n",
    "features[:, 5] = pd.Series(price).rolling(20).mean().fillna(method='bfill').values  # MA20\n",
    "features[:, 6] = pd.Series(price).rolling(50).mean().fillna(method='bfill').values  # MA50\n",
    "features[:, 7] = pd.Series(returns).rolling(20).std().fillna(method='bfill').values  # Volatility\n",
    "\n",
    "# Normalize features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "features = scaler.fit_transform(features)\n",
    "\n",
    "print(f\"Generated {n_samples} samples with {n_features} features\")\n",
    "print(f\"Price range: ${price.min():.2f} - ${price.max():.2f}\")\n",
    "print(f\"Return statistics: mean={returns.mean():.4f}, std={returns.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 10))\n",
    "\n",
    "# Price series\n",
    "axes[0].plot(dates, price, label='Price', color='blue', alpha=0.7)\n",
    "axes[0].set_ylabel('Price ($)')\n",
    "axes[0].set_title('Synthetic Financial Time Series')\n",
    "axes[0].legend()\n",
    "\n",
    "# Returns\n",
    "axes[1].plot(dates, returns, label='Returns', color='green', alpha=0.7)\n",
    "axes[1].set_ylabel('Returns')\n",
    "axes[1].axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "axes[1].legend()\n",
    "\n",
    "# Feature heatmap (first 10 features)\n",
    "im = axes[2].imshow(features[:200, :10].T, aspect='auto', cmap='coolwarm')\n",
    "axes[2].set_ylabel('Features')\n",
    "axes[2].set_xlabel('Time')\n",
    "axes[2].set_title('Feature Heatmap (First 10 Features, 200 Time Steps)')\n",
    "plt.colorbar(im, ax=axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train, validation, and test sets\n",
    "train_size = int(0.7 * n_samples)\n",
    "val_size = int(0.15 * n_samples)\n",
    "test_size = n_samples - train_size - val_size\n",
    "\n",
    "# Features and targets\n",
    "X = features\n",
    "y = returns\n",
    "\n",
    "# Split\n",
    "X_train = X[:train_size]\n",
    "y_train = y[:train_size]\n",
    "X_val = X[train_size:train_size+val_size]\n",
    "y_val = y[train_size:train_size+val_size]\n",
    "X_test = X[train_size+val_size:]\n",
    "y_test = y[train_size+val_size:]\n",
    "\n",
    "print(f\"Train: {len(X_train)} samples\")\n",
    "print(f\"Validation: {len(X_val)} samples\")\n",
    "print(f\"Test: {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare sequences for neural networks\n",
    "def prepare_sequences(X, y, seq_length=20):\n",
    "    X_seq = []\n",
    "    y_seq = []\n",
    "    \n",
    "    for i in range(seq_length, len(X)):\n",
    "        X_seq.append(X[i-seq_length:i])\n",
    "        y_seq.append(y[i])\n",
    "    \n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# Create sequences\n",
    "seq_length = 20\n",
    "X_train_seq, y_train_seq = prepare_sequences(X_train, y_train, seq_length)\n",
    "X_val_seq, y_val_seq = prepare_sequences(X_val, y_val, seq_length)\n",
    "X_test_seq, y_test_seq = prepare_sequences(X_test, y_test, seq_length)\n",
    "\n",
    "print(f\"Sequence shape: {X_train_seq.shape}\")\n",
    "print(f\"Target shape: {y_train_seq.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Individual Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LSTM model\n",
    "lstm_model = LSTMModel(\n",
    "    input_size=n_features,\n",
    "    hidden_size=64,\n",
    "    num_layers=2,\n",
    "    dropout=0.2\n",
    ")\n",
    "\n",
    "print(f\"LSTM Model Parameters: {lstm_model.count_parameters():,}\")\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = TimeSeriesDataset(X_train_seq, y_train_seq)\n",
    "val_dataset = TimeSeriesDataset(X_val_seq, y_val_seq)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Train model\n",
    "print(\"\\nTraining LSTM...\")\n",
    "lstm_history = train_sequence_model(\n",
    "    lstm_model, train_loader, val_loader,\n",
    "    epochs=50,\n",
    "    learning_rate=0.001,\n",
    "    early_stopping_patience=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GRU model\n",
    "gru_model = GRUModel(\n",
    "    input_size=n_features,\n",
    "    hidden_size=64,\n",
    "    num_layers=2,\n",
    "    dropout=0.2\n",
    ")\n",
    "\n",
    "print(f\"GRU Model Parameters: {gru_model.count_parameters():,}\")\n",
    "\n",
    "# Train model\n",
    "print(\"\\nTraining GRU...\")\n",
    "gru_history = train_sequence_model(\n",
    "    gru_model, train_loader, val_loader,\n",
    "    epochs=50,\n",
    "    learning_rate=0.001,\n",
    "    early_stopping_patience=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Transformer model\n",
    "transformer_model = TransformerPredictor(\n",
    "    input_size=n_features,\n",
    "    d_model=64,\n",
    "    n_heads=4,\n",
    "    n_layers=2,\n",
    "    d_ff=256,\n",
    "    dropout=0.1,\n",
    "    max_seq_len=seq_length\n",
    ")\n",
    "\n",
    "print(f\"Transformer Model Parameters: {transformer_model.count_parameters():,}\")\n",
    "\n",
    "# Train model\n",
    "print(\"\\nTraining Transformer...\")\n",
    "transformer_history = train_sequence_model(\n",
    "    transformer_model, train_loader, val_loader,\n",
    "    epochs=50,\n",
    "    learning_rate=0.001,\n",
    "    early_stopping_patience=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create XGBoost model\n",
    "xgb_model = XGBoostPredictor(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8\n",
    ")\n",
    "\n",
    "# Train model\n",
    "print(\"Training XGBoost...\")\n",
    "xgb_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(f\"Best iteration: {xgb_model.best_iteration_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compare Training Histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training histories\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Training loss\n",
    "axes[0].plot(lstm_history['train_loss'], label='LSTM', alpha=0.7)\n",
    "axes[0].plot(gru_history['train_loss'], label='GRU', alpha=0.7)\n",
    "axes[0].plot(transformer_history['train_loss'], label='Transformer', alpha=0.7)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].legend()\n",
    "axes[0].set_yscale('log')\n",
    "\n",
    "# Validation loss\n",
    "axes[1].plot(lstm_history['val_loss'], label='LSTM', alpha=0.7)\n",
    "axes[1].plot(gru_history['val_loss'], label='GRU', alpha=0.7)\n",
    "axes[1].plot(transformer_history['val_loss'], label='Transformer', alpha=0.7)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].set_title('Validation Loss')\n",
    "axes[1].legend()\n",
    "axes[1].set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for all models\n",
    "models = {\n",
    "    'LSTM': (lstm_model, X_test_seq),\n",
    "    'GRU': (gru_model, X_test_seq),\n",
    "    'Transformer': (transformer_model, X_test_seq),\n",
    "    'XGBoost': (xgb_model, X_test)\n",
    "}\n",
    "\n",
    "predictions = {}\n",
    "for name, (model, X_data) in models.items():\n",
    "    if name == 'XGBoost':\n",
    "        predictions[name] = model.predict(X_data)\n",
    "    else:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.FloatTensor(X_data)\n",
    "            pred = model(X_tensor).numpy().flatten()\n",
    "            predictions[name] = pred\n",
    "\n",
    "# Align test data for comparison\n",
    "y_test_aligned = y_test_seq if 'LSTM' in predictions else y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models\n",
    "evaluator = ModelEvaluator()\n",
    "results = {}\n",
    "\n",
    "for name, y_pred in predictions.items():\n",
    "    # Use appropriate test set\n",
    "    y_true = y_test_seq if name != 'XGBoost' else y_test\n",
    "    \n",
    "    metrics = evaluator.evaluate(y_true, y_pred)\n",
    "    results[name] = metrics\n",
    "    \n",
    "    print(f\"\\n{name} Results:\")\n",
    "    print(f\"  RMSE: {metrics['rmse']:.6f}\")\n",
    "    print(f\"  MAE: {metrics['mae']:.6f}\")\n",
    "    print(f\"  R²: {metrics['r2']:.4f}\")\n",
    "    print(f\"  Direction Accuracy: {metrics.get('directional_accuracy', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison visualization\n",
    "results_df = pd.DataFrame(results).T\n",
    "metrics_to_plot = ['rmse', 'mae', 'r2', 'directional_accuracy']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    ax = axes[i]\n",
    "    values = results_df[metric].values\n",
    "    models_list = results_df.index\n",
    "    \n",
    "    bars = ax.bar(models_list, values, alpha=0.7)\n",
    "    \n",
    "    # Color bars based on performance\n",
    "    if metric in ['rmse', 'mae']:  # Lower is better\n",
    "        best_idx = np.argmin(values)\n",
    "    else:  # Higher is better\n",
    "        best_idx = np.argmax(values)\n",
    "    \n",
    "    bars[best_idx].set_color('green')\n",
    "    \n",
    "    ax.set_title(f'{metric.upper()}')\n",
    "    ax.set_ylabel(metric.upper())\n",
    "    \n",
    "    # Add value labels\n",
    "    for j, v in enumerate(values):\n",
    "        ax.text(j, v + 0.001 * max(values), f'{v:.4f}', \n",
    "                ha='center', va='bottom')\n",
    "\n",
    "plt.suptitle('Model Performance Comparison', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ensemble with XGBoost and a simple Ridge model\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Train additional models for ensemble\n",
    "ridge_model = Ridge(alpha=1.0)\n",
    "ridge_model.fit(X_train, y_train)\n",
    "\n",
    "rf_model = RandomForestRegressor(n_estimators=50, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Create ensemble\n",
    "ensemble_models = {\n",
    "    'XGBoost': xgb_model,\n",
    "    'Ridge': ridge_model,\n",
    "    'RandomForest': rf_model\n",
    "}\n",
    "\n",
    "# Test different ensemble strategies\n",
    "ensemble_strategies = ['averaging', 'median', 'stacking']\n",
    "ensemble_results = {}\n",
    "\n",
    "for strategy in ensemble_strategies:\n",
    "    print(f\"\\nTesting {strategy} ensemble...\")\n",
    "    \n",
    "    if strategy == 'stacking':\n",
    "        ensemble = EnsembleModel(\n",
    "            models=ensemble_models.copy(),\n",
    "            strategy=strategy,\n",
    "            meta_learner=Ridge(alpha=0.1)\n",
    "        )\n",
    "    else:\n",
    "        ensemble = EnsembleModel(\n",
    "            models=ensemble_models.copy(),\n",
    "            strategy=strategy\n",
    "        )\n",
    "    \n",
    "    # Fit ensemble\n",
    "    ensemble.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    eval_results = ensemble.evaluate(X_test, y_test)\n",
    "    ensemble_results[f'Ensemble_{strategy}'] = eval_results['ensemble']\n",
    "    \n",
    "    print(f\"Ensemble ({strategy}) RMSE: {eval_results['ensemble']['rmse']:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost feature importance\n",
    "importance = xgb_model.get_feature_importance()\n",
    "importance_values = list(importance.values())\n",
    "importance_indices = np.argsort(importance_values)[-20:]  # Top 20\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(range(len(importance_indices)), \n",
    "         [importance_values[i] for i in importance_indices],\n",
    "         alpha=0.7)\n",
    "plt.yticks(range(len(importance_indices)), \n",
    "           [f'Feature {i}' for i in importance_indices])\n",
    "plt.xlabel('Importance Score')\n",
    "plt.title('Top 20 Feature Importances (XGBoost)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Prediction Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions for a subset of test data\n",
    "n_plot = 100\n",
    "plot_start = 0\n",
    "plot_end = plot_start + n_plot\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, (name, y_pred) in enumerate(predictions.items()):\n",
    "    if i >= 4:\n",
    "        break\n",
    "        \n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Use appropriate test set\n",
    "    y_true = y_test_seq if name != 'XGBoost' else y_test\n",
    "    \n",
    "    # Plot subset\n",
    "    x_range = range(plot_start, min(plot_end, len(y_true)))\n",
    "    ax.plot(x_range, y_true[plot_start:plot_end], \n",
    "            label='Actual', alpha=0.7, linewidth=2)\n",
    "    ax.plot(x_range, y_pred[plot_start:plot_end], \n",
    "            label='Predicted', alpha=0.7, linewidth=2)\n",
    "    \n",
    "    ax.set_title(f'{name} Predictions')\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Returns')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Model Predictions vs Actual Returns', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Walk-Forward Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform walk-forward validation for XGBoost\n",
    "print(\"Performing walk-forward validation...\")\n",
    "\n",
    "validator = WalkForwardValidator(\n",
    "    train_size=1000,\n",
    "    test_size=100,\n",
    "    step_size=50,\n",
    "    retrain_frequency=4\n",
    ")\n",
    "\n",
    "# Create trainer for walk-forward\n",
    "xgb_trainer = ModelTrainer(\n",
    "    model_type='xgboost',\n",
    "    model_config={\n",
    "        'n_estimators': 50,\n",
    "        'max_depth': 4,\n",
    "        'learning_rate': 0.1\n",
    "    },\n",
    "    training_config={},\n",
    "    use_mlflow=False\n",
    ")\n",
    "\n",
    "# Run walk-forward validation\n",
    "wf_results = validator.validate(xgb_trainer, X, y)\n",
    "\n",
    "print(f\"\\nWalk-forward validation results:\")\n",
    "print(f\"Overall RMSE: {wf_results['overall_metrics']['rmse']:.6f}\")\n",
    "print(f\"Overall MAE: {wf_results['overall_metrics']['mae']:.6f}\")\n",
    "print(f\"Overall R²: {wf_results['overall_metrics']['r2']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot walk-forward results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Predictions vs Actuals\n",
    "axes[0, 0].scatter(wf_results['actuals'], wf_results['predictions'], \n",
    "                   alpha=0.5, s=10)\n",
    "axes[0, 0].plot([min(wf_results['actuals']), max(wf_results['actuals'])],\n",
    "                [min(wf_results['actuals']), max(wf_results['actuals'])],\n",
    "                'r--', lw=2)\n",
    "axes[0, 0].set_xlabel('Actual')\n",
    "axes[0, 0].set_ylabel('Predicted')\n",
    "axes[0, 0].set_title('Walk-Forward: Predictions vs Actuals')\n",
    "\n",
    "# Time series of predictions\n",
    "axes[0, 1].plot(wf_results['actuals'], label='Actual', alpha=0.7)\n",
    "axes[0, 1].plot(wf_results['predictions'], label='Predicted', alpha=0.7)\n",
    "axes[0, 1].set_xlabel('Time')\n",
    "axes[0, 1].set_ylabel('Value')\n",
    "axes[0, 1].set_title('Walk-Forward: Time Series')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Rolling window metrics\n",
    "window_mse = [m['mse'] for m in wf_results['metrics']]\n",
    "window_starts = [m['window_start'] for m in wf_results['metrics']]\n",
    "axes[1, 0].plot(window_starts, window_mse, marker='o', alpha=0.7)\n",
    "axes[1, 0].set_xlabel('Window Start')\n",
    "axes[1, 0].set_ylabel('MSE')\n",
    "axes[1, 0].set_title('Rolling Window MSE')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals\n",
    "residuals = np.array(wf_results['actuals']) - np.array(wf_results['predictions'])\n",
    "axes[1, 1].hist(residuals, bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[1, 1].axvline(x=0, color='red', linestyle='--')\n",
    "axes[1, 1].set_xlabel('Residual')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('Residual Distribution')\n",
    "\n",
    "plt.suptitle('Walk-Forward Validation Results', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final summary\n",
    "all_results = {**results, **ensemble_results}\n",
    "summary_df = pd.DataFrame(all_results).T[['rmse', 'mae', 'r2', 'directional_accuracy']]\n",
    "summary_df = summary_df.sort_values('rmse')\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nModels ranked by RMSE (lower is better):\")\n",
    "print(summary_df.to_string())\n",
    "\n",
    "# Best model\n",
    "best_model = summary_df.index[0]\n",
    "print(f\"\\nBest performing model: {best_model}\")\n",
    "print(f\"  RMSE: {summary_df.loc[best_model, 'rmse']:.6f}\")\n",
    "print(f\"  R²: {summary_df.loc[best_model, 'r2']:.4f}\")\n",
    "print(f\"  Direction Accuracy: {summary_df.loc[best_model, 'directional_accuracy']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
